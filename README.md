[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)
[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)
![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)
[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)
[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://discord.gg/pAbnFJrkgZ)

# LLM Question Answering App ğŸš€
## Overview ğŸ“š
The LLM Question Answering App is a cutting-edge web application designed to make text file processing and querying simple and efficient. Built with Streamlit, this app allows users to upload text files (docx, pdf, txt) and convert them into embeddings stored in ChromaDB. This innovative approach enables users to query documents for specific information, making it an invaluable tool for extracting insights from large volumes of text.
## Features ğŸŒŸ
â€¢	Upload and Process Text Files: Effortlessly upload text files in various formats and process them within the app. ğŸ“

â€¢	Text to Embeddings Conversion: Transform uploaded text into embeddings, enhancing searchability and accessibility. ğŸ§ 

â€¢	ChromaDB Storage: Store embeddings in ChromaDB, a vector store optimized for efficient querying. ğŸ’¾

â€¢	Query Documents: Ask questions related to the document, and the app will provide relevant answers extracted directly from the document. ğŸ”

## Getting Started ğŸ
### Prerequisites
Ensure you have Python version 3.7 or higher installed on your system. ğŸ
### Installation
1.	Clone the Repository: Clone this repository to your local machine.
```bash
git clone https://github.com/yourusername/llm-question-answering-app.git
```


2.	Install Dependencies: Navigate to the project directory and install the required Python libraries.
```bash
pip install -r requirements.txt
```



### Running the App
1.	Start the Streamlit Server: Run the Streamlit app using the following command.
```bash
streamlit run app.py
```


2.	Access the App: Open your web browser and navigate to the URL provided by Streamlit (usually http://localhost:8501). ğŸŒ
## Deployment
For deploying your app to the Streamlit Community Cloud, follow these steps:
1.	Navigate to Streamlit Community Cloud: Go to the Streamlit Community Cloud and click the "New app" button.
2.	Choose Repository, Branch, and Application File: Select the appropriate repository, branch, and application file to deploy your app.
## Contributing ğŸ¤
Contributions are welcome! Please feel free to submit a pull request or open an issue if you find any bugs or have suggestions for improvements. ğŸ“¬
License
This project is licensed under the MIT License. See the LICENSE file for details. ğŸ“„
________________________________________
This README provides a comprehensive overview of the LLM Question Answering App, including setup instructions, features, and deployment guidelines. It's designed to help users quickly understand how to use and contribute to the project.

